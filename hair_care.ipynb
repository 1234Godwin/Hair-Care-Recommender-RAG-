{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Base blog page\n",
    "base_url = \"https://skinkraft.com/blogs/articles\"\n",
    "\n",
    "# Send HTTP request\n",
    "response = requests.get(base_url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Extract all article links\n",
    "article_links = []\n",
    "for a_tag in soup.find_all(\"a\", href=True):\n",
    "    href = a_tag[\"href\"]\n",
    "    full_url = urljoin(base_url, href)\n",
    "    if \"/blogs/articles/\" in full_url and full_url not in article_links:\n",
    "        article_links.append(full_url)\n",
    "\n",
    "print(f\"‚úÖ Found {len(article_links)} article links.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base path to crawl from\n",
    "base_url = \"https://www.aad.org/public/everyday-care/hair-scalp-care/hair/\"\n",
    "\n",
    "# Fetch and parse page\n",
    "response = requests.get(base_url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Extract all <a href> that start with base path\n",
    "hair_links = []\n",
    "for a_tag in soup.find_all(\"a\", href=True):\n",
    "    href = a_tag[\"href\"]\n",
    "    full_url = urljoin(base_url, href)\n",
    "\n",
    "    # Keep only links under the same hair care path\n",
    "    if full_url.startswith(base_url) and full_url not in hair_links:\n",
    "        hair_links.append(full_url)\n",
    "\n",
    "print(f\"‚úÖ Found {len(hair_links)} hair-related URLs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Starting page\n",
    "base_page = \"https://en.wikipedia.org/wiki/Hair_care\"\n",
    "\n",
    "response = requests.get(base_page)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Collect internal Wikipedia links that include 'hair'\n",
    "hair_links = []\n",
    "for a in soup.find_all(\"a\", href=True):\n",
    "    href = a[\"href\"]\n",
    "    if href.startswith(\"/wiki/\") and \"hair\" in href.lower():\n",
    "        full_url = urljoin(\"https://en.wikipedia.org\", href)\n",
    "        hair_links.append(full_url)\n",
    "\n",
    "hair_urls = hair_links.copy()\n",
    "del hair_urls[2:7]\n",
    "del hair_urls[2]\n",
    "del hair_urls[12]           # Single item at index 12 (after first deletion, list is now shorter)\n",
    "del hair_urls[-11]          # Index from the end\n",
    "del hair_urls[-11]          # Same position after previous removal\n",
    "del hair_urls[-23]          # Index from the end\n",
    "del hair_urls[-24]          # Index from the end\n",
    "del hair_urls[-8:-4]        # Slice near the end\n",
    "del hair_urls[-1]\n",
    "del hair_urls[7]\n",
    "del hair_urls[1]\n",
    "del hair_urls[5]\n",
    "del hair_urls[3]\n",
    "del hair_urls[-8]\n",
    "del hair_urls[-6]\n",
    "\n",
    "hair_urls[-5] = 'https://en.wikipedia.org/wiki/Human_hair_growth'\n",
    "hair_urls.extend(['https://www.verywellhealth.com/wet-dandruff-treatment-5197087',\n",
    "           'https://www.self.com/story/best-dandruff-shampoos',\n",
    "            'https://www.allure.com/story/the-science-of-beauty-the-complete-guide-to-scalp-care'])\n",
    "\n",
    "\n",
    "print(f\"‚úÖ Found {len(hair_links)} hair-related URLs on Hair care page.\")\n",
    "print(f\"‚úÖ {len(hair_urls)} collated URLs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this combines all the urls retrieved from both websites\n",
    "all_scraped_urls = hair_links + article_links + hair_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "def fetch_clean_text_from_url(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=8)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Remove unwanted tags\n",
    "        for tag in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\", \"aside\"]):\n",
    "            tag.decompose()\n",
    "\n",
    "        text = soup.get_text(separator=\" \")\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "        return Document(page_content=text, metadata={\"source\": url})\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to fetch {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Scrape all documents from URLs\n",
    "web_docs = [doc for url in all_scraped_urls if (doc := fetch_clean_text_from_url(url))]\n",
    "\n",
    "print(f\"‚úÖ Total successfully scraped documents: {len(web_docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # normalize whitespace\n",
    "    text = re.sub(r'[‚Äú‚Äù]', '\"', text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9.,;:'\\\"()?!\\s-]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "directory_path = r\"C:\\Users\\ADMIN\\Desktop\\Chiemelie\\Data_Science\\final_students_projects\\hair_recommender_project\\document_folder\"\n",
    "loader = PyPDFDirectoryLoader(directory_path)\n",
    "local_docs = loader.load()\n",
    "\n",
    "# Clean loaded local_docs\n",
    "for doc in local_docs:\n",
    "    doc.page_content = clean_text(doc.page_content)\n",
    "\n",
    "print(f\"‚úÖ Loaded and cleaned {len(local_docs)} documents from {directory_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine both sources\n",
    "all_docs = local_docs + web_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True\n",
    ")\n",
    "\n",
    "all_splits = splitter.split_documents(all_docs)\n",
    "print(f\"üß© Total document chunks: {len(all_splits)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=r\"C:\\Users\\ADMIN\\Desktop\\Chiemelie\\Data_Science\\final_students_projects\\huggingface\\all-mpnet-base-v2\"\n",
    ")\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"hair_recommender_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=r\"C:\\Users\\ADMIN\\Desktop\\Chiemelie\\Data_Science\\final_students_projects\\hair_recommender_project\\hair_care_chroma_db\"\n",
    ")\n",
    "\n",
    "vector_store.add_documents(all_splits)\n",
    "\n",
    "print(\"‚úÖ Vector store successfully built and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import re\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# --- Load FLAN-T5 Model ---\n",
    "model_path = r\"C:\\Users\\ADMIN\\Desktop\\Chiemelie\\Data_Science\\final_students_projects\\hair_recommender_project\\flan_t5_large_local\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "\n",
    "# --- Truncation Helper ---\n",
    "def truncate_input(text, tokenizer, max_tokens=512):\n",
    "    # Truncate directly using tokenizer.encode and decode\n",
    "    tokens = tokenizer.encode(text, truncation=True, max_length=max_tokens)\n",
    "    return tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "\n",
    "# --- HuggingFace Pipeline ---\n",
    "generator = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=256,  # Output length\n",
    "    repetition_penalty=1.2,\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=generator)\n",
    "\n",
    "# --- Build RAG Chain with Retriever ---\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vector_store.as_retriever(search_type=\"mmr\", search_kwargs={'k': 2}),\n",
    "    return_source_documents=True\n",
    ")\n",
    "print(\"‚úÖ RetrievalQA pipeline successfully loaded.\")\n",
    "\n",
    "# --- System Prompt ---\n",
    "system_prompt = (\n",
    "    \"You are an expert hair care assistant. \"\n",
    "    \"Given the context from trusted medical or cosmetic sources, \"\n",
    "    \"provide a clear, medically accurate, and helpful response. \"\n",
    "    \"Avoid repeating the same text, remove unnecessary quotes, and make sure the answer is human-readable. \"\n",
    "    \"Do not hallucinate. If the answer is not found in the context, reply: 'I'm not sure based on the current information.'\"\n",
    ")\n",
    "print('all ran successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import language_tool_python\n",
    "\n",
    "# --- Prompt Formatter ---\n",
    "def format_query(query, context_docs):\n",
    "    combined_input = (\n",
    "        system_prompt.strip() + \"\\n\\n\"\n",
    "        + \"Context:\\n\" + context_docs.strip() + \"\\n\\n\"\n",
    "        + \"Question: \" + query.strip()\n",
    "    )\n",
    "    return truncate_input(combined_input, tokenizer)\n",
    "\n",
    "# --- Answer Cleaning ---\n",
    "def clean_answer(text):\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)  # Remove double spaces\n",
    "    text = re.sub(r\"\\(.*?\\)\", \"\", text)  # Remove parentheses content\n",
    "    text = re.sub(r\"[\\\"\\'\\[\\]`]\", \"\", text)  # Strip extra quotation marks and brackets\n",
    "    text = re.sub(r\"\\s+([.,!?])\", r\"\\1\", text)  # Clean spacing before punctuation\n",
    "    text = re.sub(r\"(?<!\\w)\\s*[A-Z]{2,}\\s*(?!\\w)\", \"\", text)  # Remove lone ALL CAPS\n",
    "    return text.strip().capitalize()\n",
    "\n",
    "\n",
    "def shorten_context(docs, max_chars=2000):\n",
    "    combined = \"\"\n",
    "    for doc in docs:\n",
    "        if len(combined) + len(doc.page_content) > max_chars:\n",
    "            break\n",
    "        combined += doc.page_content + \"\\n\\n\"\n",
    "    return combined.strip()\n",
    "\n",
    "\n",
    "\n",
    "tool = language_tool_python.LanguageToolPublicAPI('en-US')\n",
    "\n",
    "def correct_grammar(text):\n",
    "    matches = tool.check(text)\n",
    "    return language_tool_python.utils.correct(text, matches)\n",
    "\n",
    "def fix_common_contractions(text):\n",
    "    text = re.sub(r\"\\bIm\\b\", \"I'm\", text)\n",
    "    text = re.sub(r\"\\bi\\b\", \"I\", text)\n",
    "    text = re.sub(r\"\\bdont\\b\", \"don't\", text)\n",
    "    text = re.sub(r\"\\bdoesnt\\b\", \"doesn't\", text)\n",
    "    # Add more contractions as needed\n",
    "    return text\n",
    "\n",
    "\n",
    "def ask_rag(query):\n",
    "    retrieved_docs = rag_chain.retriever.invoke(query)\n",
    "\n",
    "    # Shorten the context to avoid long input\n",
    "    context = shorten_context(retrieved_docs, max_chars=2000)\n",
    "\n",
    "    # Format and truncate final input\n",
    "    formatted_query = format_query(query, context)\n",
    "\n",
    "    # Generate answer\n",
    "    result = rag_chain.invoke(formatted_query)\n",
    "\n",
    "    # Postprocess answer\n",
    "    answer = clean_answer(result[\"result\"])\n",
    "    answer = fix_common_contractions(answer)         # <-- Add this\n",
    "    answer = correct_grammar(answer)                 # <-- And this\n",
    "\n",
    "    # Print\n",
    "    print(\"üß† Answer:\", answer)\n",
    "    print(\"\\nüìö Sources:\")\n",
    "    for doc in result[\"source_documents\"]:\n",
    "        print(\"-\", doc.metadata.get(\"source\", \"No source\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hair_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
